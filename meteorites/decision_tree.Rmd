---
title: "Meteors Continued, Again"
subtitle: Decision Tree
output:
  html_document:
    df_print: paged
  pdf_document: default
author: "Lesley Duff"
date: "`r format(Sys.Date())`"
---
```{r}
#set seed to ensure reproducible results
set.seed(20)
```

# Decision Tree
```{r}
library(modelr)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(yardstick)
```

This is a mini homework for people doing the PDA. You are going to continue 
working on your week 2 weekend homework.

Based on the mass and year, create a decision tree model of your meteorites 
data which predicts whether a meteorite was seen falling, or was discovered 
after its impact (fell/found).

First, you want to convert any character column to a factor. Next (and most 
importantly) split your dataset into training and testing datasets. Then 
create a decision tree with rpart and plot it using rpart.plot. Finally, test 
and add your predictions to your data, and create a confusion matrix.

```{r}
# Load clean meteorite data
meteorite_landings <- read_csv("clean_data/meteorite_landings.csv")
```

```{r}
# View(meteorite_landings)

# Meterorites are ordered by year shuffle them
shuffle_index <- sample(1:nrow(meteorite_landings))

# shuffle the data so class order isn't in order - need this for training/testing split later on
meteorite_landings_shuffled <- meteorite_landings[shuffle_index, ]

# First, you want to convert any character column to a factor.
meteorite_landings_factored <- meteorite_landings_shuffled %>%
  mutate(across(where(is.character), as.factor))

head(meteorite_landings_factored)

skimr::skim(meteorite_landings_factored)

meteorite_landings_factored <- meteorite_landings_factored %>%
  # We still have some missing entries for year lets clean them
  na.omit() %>%
  # remove some unneeded columns
  select(-id, -name)

glimpse(meteorite_landings_factored)
```
Split dataset into training and testing datasets.

```{r}
# 80% of our data going into the training set, and the
# remaining 20% go in our test set.

# get how many rows we have in total to work out the percentage
n_data <- nrow(meteorite_landings_factored)

# create a test sample index
test_index <- sample(1:n_data, size = n_data * 0.2)

# 4.2 Testing datasets (Week 11)
# create test set
meteorite_landings_test <- slice(meteorite_landings_factored, test_index)

# 4.3 Training datasets (Week 11)
# create training set
meteorite_landings_train <- slice(meteorite_landings_factored, -test_index)
```

Check that our test and training sets have similar proportions of 
fall. 

```{r}
# 4.2 Testing datasets (Week 11)
meteorite_landings_test %>%
  janitor::tabyl(fall)
```

```{r}
# 4.3 Training datasets (Week 11)
meteorite_landings_train %>%
  janitor::tabyl(fall)
```

Build tree model based on training dataset.

Then create a decision tree with rpart.

```{r}
# 4.3 Training datasets (Week 11)
meteorite_landings_fit <- rpart(
  # ?rpart
  # Based on the mass and year
  # mass_g and year,
  formula = fall ~ mass_g + year,
  data = meteorite_landings_train,
  method = "class"
)
```

Plot it using rpart.plot

```{r}
#?rpart.plot

# 4.3 Training datasets (Week 11)
rpart.plot(meteorite_landings_fit,
  yesno = 2,
  fallen.leaves = TRUE,
  faclen = 6,
  digits = 2
)
```

### Interpretation of decision tree

This meteorite model is a binary response Found/Fell. Each node shows the 
feature (predictor variable) and value used for the split e.g. for the root node (Found, 0.85, 100%). The second line contains probability of a Found 
result expressed as a decimal i.e. 0.85, the third line is the percentage of data points which pass through this node.

From root node where data meets the split `year < 1935` for 
years older than 1935, it follows the left branch, years from 1935 onwards it follows the right branch. Once you reach a leaf node,  the node tells you the predicted outcome, in this case 'Fell' or 'Found'.

So from root if year was >= 1935 we'd take the right branch to
Found, 0.93, 80%. This tells us a very high percentage of meteorites discovered 1935 or later are predicted to have been discovered **after** its impact than seen falling.

For the left branch from root

1. Root Node take 'year < 1935'(dark green) - take left branch
2. Node: Found, 0.57, 20% (light green)
The split is now on mass rather than year.
For mass in grammes < 11000 (11e+3) take left branch
3. Node: Fell, 0.40, 10% (light blue)
For the first time we see 'Fell' as predicted more common. The split is now on
year < 1880
4. Leaf Node: Fell, 0.21, 4%. (dark blue)
So Fell is predicted to be more likely for lighter meteorites < 11000g, where the year is older than 1880 

## Predictions

Finally, test and add your predictions to your data

```{r}
# 4.2 Testing datasets (Week 11)
# add the predictions
meteorite_landings_test_pred <- meteorite_landings_test %>%
  add_predictions(meteorite_landings_fit, type = "class")

# look at the variables  choose the ones that our decision tree model showed
# as most informative.
# names(meteorite_landings_test)

meteorite_landings_test_pred %>%
  select(year, mass_g, fall, pred)
```

## Confusion matrix

Check predictive performance by creating a confusion matrix.

```{r}
#?conf_mat
# 4.2 Testing datasets (Week 11)
conf_mat <- meteorite_landings_test_pred %>%
  conf_mat(truth = fall, estimate = pred)

conf_mat
```

### Accuracy

```{r}
accuracy <- meteorite_landings_test_pred %>%
 accuracy(truth = fall, estimate = pred)

accuracy 
```
### Sensitivity

```{r}
sensitivity <- meteorite_landings_test_pred %>%
  sensitivity(truth = fall, estimate = pred)

sensitivity
```
### Specificity

```{r}
specificity <- meteorite_landings_test_pred %>%
  specificity(truth = fall, estimate = pred)

specificity
```
### Interpretation of confusion matrix

The main diagonal represents correctly-predicted values, with the top right 
values showing false positives and the bottom left being false negatives.
So where 'Fell' was predicted this was true 55 times (True Positive), false 23 
times (False Positive).

Prediction of Found and was actually Fell 120 is a False Negative

Our correctly predicted values are higher than the incorrect ones. This is
reflected in the accuracy - the probability of our prediction being correct as 
over around 85%.

Our sensitivity (AKA true positive rate) of around 31% is the proportion of 
actual positive cases that are correctly identified as positive.

Prediction of Found and was actually Found 764 is a True Negative, this value 
is very high.
Our specificity (AKA true negative rate) of over 97% proportion of actual 
negative cases that are correctly identified as negative.

The model is better at predicting when the meteorite is found rather than fell.






